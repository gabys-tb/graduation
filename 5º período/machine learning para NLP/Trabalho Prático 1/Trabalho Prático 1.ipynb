{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho Prático 1 - Aprendizado Profundo para NLP\n",
    "\n",
    "__Aluna:__ Gabriela Tavares Barreto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Introdução__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O objetivo deste trabalho prático foi realizar uma avaliação intrínseca de modelos de palavras distribuídas. O método escolhido for O Continuous Bag of Words (CBOW). O CBOW é um método de treinamento de modelos de linguagem em aprendizado de máquina, especificamente usado para aprender representações distribuídas de palavras em um corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Instalando módulos__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação foi feita utilizando a biblioteca gensim para criar o modelo CBOW. O código utilizado está disponível no repositório gensym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (6.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Pré-processamento do corpus__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O corpus utilizado para treinar o modelo foi obtido a partir do conjunto de dados text8 disponível em https://mattmahoney.net/dc/text8.zip. O corpus foi pré-processado para remover pontuações, converter para minúsculas, entre outros procedimentos, visando obter melhores representações distribuídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Tokenizando o corpus:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Carregando o corpus de texto\n",
    "with open('text8.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus_text = file.read()\n",
    "# Tokenizando o corpus\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in nltk.sent_tokenize(corpus_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Divindo o corpus em sentenças:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, chunk_size):\n",
    "    \"\"\"Divide uma lista em listas menores com tamanho especificado.\"\"\"\n",
    "    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]\n",
    "\n",
    "chunk_size = 1000 # tamanho da sentença\n",
    "\n",
    "tokenized_corpus_final = chunk_list(tokenized_corpus[0], chunk_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. Escolhas de hiperparâmetros__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Foram realizadas várias escolhas durante o processo de treinamento, incluindo:\n",
    "\n",
    "__Tamanho do Treinamento:__ Diferentes tamanhos de treinamento foram experimentados para avaliar o impacto no desempenho do modelo.\n",
    "\n",
    "__Tamanho da Janela (Window Size):__ O tamanho da janela, que determina o contexto ao redor de uma palavra, foi ajustado para explorar a influência dessa variável na qualidade das representações.\n",
    "\n",
    "__CBOW vs Skip-Gram:__ A escolha entre CBOW e Skip-Gram, dois métodos populares de treinamento de modelos Word2Vec, foi investigada para entender as diferenças e semelhanças em termos de desempenho intrínseco.\n",
    "\n",
    "__Tamaho do vetor:__ A dimensão do vetor de embedding tem um impacto crucial na qualidade da representação das palavras. Assim vários tamanhos foram testados levando em conta o trade-off entre qualidade de representação e custo computacional de processamento do modelo. \n",
    "\n",
    "__Min count:__ A abordagem adotada para determinar o valor adequado de min_count envolveu uma análise exploratória da distribuição de frequência das palavras no corpus. Foram realizados experimentos variando o min_count em uma ampla faixa de valores, desde permitir palavras muito raras até restringir apenas às mais frequentes. A métrica de desempenho, como a acurácia nas tarefas de avaliação intrínseca, foi monitorada para entender o impacto dessa escolha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Treinando o modelo__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Usando o CBOW para gerar embeddings de $500$ dimensões:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_corpus_final, vector_size=500, window=20, min_count=10, max_vocab_size=None, sg=0 , negative = 10, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corfu', 0.5142543911933899),\n",
       " ('hellenic', 0.5132253170013428),\n",
       " ('crete', 0.5013296008110046),\n",
       " ('cyprus', 0.49881598353385925),\n",
       " ('turkey', 0.4972730278968811),\n",
       " ('thessaloniki', 0.496123343706131),\n",
       " ('ionian', 0.48786014318466187),\n",
       " ('athens', 0.48413944244384766),\n",
       " ('larissa', 0.48275554180145264),\n",
       " ('thrace', 0.47627192735671997)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"greece\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __6. Avaliando o modelo__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação intrínseca foi realizada usando analogias com base no conjunto de dados disponível em https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt. O procedimento envolveu a inserção de três palavras, a seleção da palavra retornada pelo modelo, e o cálculo da distância para a palavra correta. Este processo foi repetido e a média das distâncias foi computada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.34%\n"
     ]
    }
   ],
   "source": [
    "score, sections = model.wv.evaluate_word_analogies('questions-words.txt')\n",
    "print(f'Accuracy: {score*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __7. Resultados & Conclusão__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Os resultados obtidos indicam a eficácia do modelo CBOW em lidar com o conjunto de dados específico. A variação de parâmetros foi crucial para otimizar o desempenho do modelo, destacando a importância da experimentação sistemática.\n",
    "\n",
    "Este trabalho me proporcionou uma compreensão prática do uso do CBOW para treinar modelos de palavras distribuídas. A escolha adequada de parâmetros foi crucial para otimizar o desempenho do modelo, e a avaliação intrínseca forneceu uma métrica valiosa para comparar diferentes configurações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __8. Referências__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gensim Documentation: https://radimrehurek.com/gensim/\n",
    "\n",
    "Nicholas Leonard - Word2Vec: https://github.com/nicholas-leonard/word2vec\n",
    "\n",
    "Material usado em sala."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
